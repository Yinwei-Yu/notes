Time:2025-10-30

Tags:[distributed](3-tags/distributed.md)

# Introduction

Spanner 是 Google 推出的**可扩展、多版本、全球分布式且同步复制的数据库**，首次实现全球范围数据分布并支持**外部一致性的分布式事务**，其核心支撑是创新的**TrueTime API**（通过 GPS 和原子钟将时钟不确定性控制在通常小于 10ms）；它以**目录（Directory）为数据放置和移动的基本单位**，基于 Paxos 协议实现复制，支持半关系型数据模型、SQL 查询语言及原子模式变更，在性能上，单副本写入延迟约 14.4ms，快照读吞吐量随副本数增加近乎线性增长（5 副本时达 50.0±1.1 Kops/sec），且已成功应用于 Google 广告后端 F1，解决了原 MySQL 分片架构的运维难题。


![](2-source-material/images/Pasted%20image%2020251030211116.png)

# Implementation

universe->全局管理数据
测试,发布,产品

zones组织
    zones是bigtable粗略模拟
    管理和部署的单位
    数据复制的不同地点
    作为新数据库加入或移除
    物理隔离的单元
    多个

![](2-source-material/images/Pasted%20image%2020251030212422.png)

## software stack

![](2-source-material/images/Pasted%20image%2020251030214238.png)
### 1. 底层核心：Tablet 数据结构

|维度|具体内容|
|---|---|
|管理范围|每个 spanserver 管 100-1000 个 Tablet|
|键值映射|(key:string, timestamp:int64)→string（与 Bigtable 一致）|
|与 Bigtable 差异|自动为数据分配时间戳→支持多版本，更接近多版本数据库（非纯键值存储）|
|存储依赖|类 B 树文件（数据组织）+ 预写日志（持久化）→ 基于 Colossus（GFS 继任者）存储|

### 2. 复制机制：Paxos 状态机

|设计点|细节与说明|
|---|---|
|与 Tablet 绑定关系|当前：1 个 Tablet→1 个 Paxos 状态机（早期多状态机因复杂被弃用）|
|状态存储|Paxos 元数据 + 日志→存储在对应 Tablet 中（数据与复制状态关联）|
|Leader 租约|长租约（默认 10 秒）→减少选举频率，提升稳定性，支撑锁表管理|
|写日志双写|Paxos 写操作→记 Tablet 日志 + Paxos 日志（临时方案，后续优化）|
|流水线化优化|支持流水线化 Paxos→重叠 WAN 延迟，提升吞吐量；严格按顺序应用写操作（Section 4 依赖）|
|Paxos 组规则|写：必须在 Leader 发起（Paxos 协议保一致）；读：任意 “足够新” 副本直接读 Tablet|

### 3. 并发控制：锁表（两阶段锁）

|维度|具体内容|
|---|---|
|部署位置|仅 Paxos Leader 副本（非 Leader 无需并发决策）|
|核心功能|键范围→锁状态映射（两阶段锁状态存储）→减少分布式场景锁冲突|
|依赖基础|Leader 长租约→稳定维护锁表状态，避免频繁重建|
|设计依据|针对长事务（如分钟级报表）→悲观锁（加锁避免冲突），优于乐观锁（冲突回滚浪费）|
|适用场景|事务性读等需同步操作→加锁；非事务读→绕过锁表|

### 4. 事务管理：事务管理器（两阶段提交）

|设计点|细节与说明|
|---|---|
|部署位置|仅 Paxos Leader 副本|
|角色划分|参与者组：participant leader/slaves；跨组事务：选协调组→coordinator leader/slaves|
|事务处理逻辑|单 Paxos 组：锁表 + Paxos→无需事务管理器；多组：协调者 Leader 执行 2PC（准备→提交 / 回滚）|
|状态保障|事务管理器状态→存储在 Paxos 组中，复制到所有副本→故障后可恢复|

## directories and placement

### 1. 目录基础定义

- 本质：共享公共前缀的连续键集合，术语 “Directory” 是历史偶然，更合适的是 “Bucket”。
- 核心价值：应用通过设计键前缀，将关联数据归为同一目录，控制数据 locality，减少跨节点访问延迟。
- 前缀来源：后续 Section 2.3（数据模型）会详细说明。

### 2. 目录的核心作用 —— 数据管理的关键单位

- 复制配置单位：目录内所有数据共享相同复制策略（副本数、区域等），无差异化配置，简化管理且保证策略统一。
- 跨 Paxos 组移动单位：
    - 移动场景：减负载（过载组→空闲组）、聚合高频目录（高频访问目录→同一组）、贴近访问源（目录→靠近客户端的组）。
    - 迁移特性：支持在线迁移（不中断客户端操作），50MB 目录迁移耗时数秒。
- 影响 Tablet 结构：
    - 区别于 Bigtable：Bigtable 的 Tablet 是 “单一连续分区”，Spanner 的 Tablet 是 “多分区容器”。
    - 设计目的：让高频访问的多个目录共置在同一 Tablet/ Paxos 组，优化 locality。

### 3. 目录迁移实现 ——Movedir 后台任务

- 核心功能：① 跨 Paxos 组迁移目录 / 片段；② 为 Paxos 组增删副本（因未支持 Paxos 内配置变更）。
- 迁移策略（非事务化，避免阻塞）：
    1. 注册迁移任务，记录 “开始迁移” 元数据。
    2. 后台异步迁移大部分数据，客户端正常读写。
    3. 剩余少量 “名义数据” 用事务原子迁移，同时更新两个 Paxos 组的元数据。

### 4. 目录复制配置 —— 职责分工

- 管理员：
    - 负责维度：副本的数量 / 类型、副本地理分布。
    - 操作：创建命名配置选项（如 “北美 5 副本含 1 见证”“欧洲 3 副本”）。
- 应用：
    - 负责维度：选择复制策略，绑定到数据库或单个目录。
    - 示例：用户 A 目录→“欧洲 3 副本”，用户 B 目录→“北美 5 副本”。

### 5. 实际优化 —— 大目录分片（Fragment）

- 触发条件：目录规模过大（超单节点承载能力）。
- 分片结果：拆分为多个 Fragment，可分配到不同 Paxos 组（不同服务器），实现横向扩展。
- 迁移适配：Movedir 实际迁移 Fragment，而非完整目录，支撑大目录高效管理。

## data model

### 核心特性与设计动因

1. 面向应用的三大数据特性

- 数据模型：基于 schematized 半关系型表，有行、列和版本化值（每个版本自动带提交时间戳）
- 查询语言：SQL 类语言，支持协议缓冲区（protocol-buffer）类型字段扩展
- 事务支持：通用事务，可跨行、跨 Paxos 组，解决 Bigtable 无跨行事务的痛点

2. 设计驱动因素

- 参考 Megastore：Megastore 因半关系型模型易管理、支持跨数据中心同步复制（Bigtable 仅最终一致），被 Gmail、Picasa 等至少 300 个 Google 应用使用，但性能低，Spanner 继承其模型并优化性能
- 跟进 Dremel：Dremel 作为交互式分析工具，因类 SQL 查询受欢迎，故 Spanner 设计 SQL 类查询语言
- 解决 Bigtable 问题：Bigtable 无跨行事务致用户抱怨多，Spanner 通过 “Paxos 上运行 2PC” 支持通用事务，用 Paxos 高可用性弥补传统 2PC 缺陷

### 层级结构（从顶层到存储底层）

1. 宇宙（Universe）：最高部署单元，Google 仅少量（测试 / 开发 / 生产各 1 个），含多个数据库
2. 数据库（Database）：应用在宇宙内创建的容器，可含无限个 schematized 表
3. 表（Table）：核心存储单元，底层基于 “目录分桶的键值映射”，表结构类似关系数据库但有差异，每行有多个版本（带时间戳）

### 关键约束（半关系型与键值存储的平衡）

1. 强制主键要求

- 每个表必须有 1 个及以上有序主键列，主键形成行的 “名称”，表本质是 “主键列→非主键列” 的映射
- 行存在规则：仅当主键列有值（即使 NULL）时行存在，非主键列无值不影响行存在
- 核心作用：应用可通过设计主键前缀控制数据 locality，让关联数据归为同一目录，减少跨节点延迟

2. 版本化值管理

- 每行数据的列值支持多版本，每个版本自动打事务提交时间戳，无需应用手动管理
- 支持场景：读取历史快照（如指定时间戳查过去数据）、历史数据审计

### 表的关联机制（INTERLEAVE IN 语法）

1. 语法作用：建立表的层级关系，让关联表行共置为目录，提升分布式查询性能
2. 规则细节

- 父表需声明为 “目录表（DIRECTORY）”，是层级顶层（如 Users 表）
- 子表主键必须包含父表主键（如 Albums 表主键 (uid, aid) 含 Users 表 uid）
- ON DELETE CASCADE：删除父表行时，自动删除关联子表行（避免数据冗余）

3. 与目录的绑定

- 父表某行（主键 K）+ 子表中主键以 K 为字典序前缀的所有行，共同构成一个目录
- 目录作为数据放置单位，存于同一 Paxos 组（相近物理节点），减少跨节点访问（如用户 uid=2 的 Users 行与 Albums 行共置）

### 核心优势

1. 易用性 + 性能：继承 Megastore 易用性、Dremel 查询友好性，基于 Paxos 优化底层提升性能
2. 自主控制 locality：通过主键前缀、INTERLEAVE IN 语法，应用可定义数据目录归属，适配全球分布式场景（如欧洲用户数据放欧洲目录）
3. 强一致 + 高可用：支持跨数据中心同步复制，Paxos 上运行 2PC，既保证外部一致性，又避免传统 2PC 协调者故障风险
4. 多版本管理：自动打时间戳，支持历史快照读取、数据审计，无需应用手动维护版本

# TrueTime
## TrueTime的核心定位与API定义

1. 定位：TrueTime是Spanner实现外部一致性、无锁只读事务等核心特性的**关键基础**，是一套直接暴露时钟不确定性的分布式时钟API，区别于传统隐藏时钟误差的时间接口，其设计目标是让分布式系统能基于可信的时间语义实现强一致性。
2. 核心数据类型：
   - TTinterval：表示时间区间，包含`earliest`（最早可能时间）和`latest`（最晚可能时间）两个端点，用于量化时钟不确定性，真实绝对时间一定落在该区间内。
   - TTstamp：TTinterval端点的时间类型，与UNIX时间类似（包含闰秒平滑处理）。
3. 三大核心API（对应Table 1）：
   - `TT.now()`：返回当前时间的TTinterval，保证调用该方法的绝对时间在`[earliest, latest]`区间内。
   - `TT.after(t)`：便捷方法，若参数`t`（TTstamp类型）已确定落在过去，返回`true`。
   - `TT.before(t)`：便捷方法，若参数`t`已确定尚未到来，返回`true`。
4.  形式化保证：设事件`e`的绝对时间为`t_abs(e)`，对`TT.now()`的调用结果`tt`，满足`tt.earliest ≤ t_abs(e_now) ≤ tt.latest`（`e_now`为调用`TT.now()`的事件）。

## TrueTime的实现架构与误差控制

1. 底层时间源：采用**GPS+原子钟**双源备份，利用两类时间源的互补故障模式保证可靠性：
   - GPS：优势是时间精度高，缺陷是易受天线故障、本地无线电干扰、信号欺骗、系统中断等影响。
   - 原子钟：优势是故障模式与GPS不相关（避免同因故障），缺陷是长期运行会因频率误差产生显著漂移。
2. 部署结构：
   - 时间主节点（per datacenter）：多数主节点配备带独立天线的GPS接收器（物理上分散部署，减少集中故障风险），少数为“末日主节点”（Armageddon masters），配备原子钟，成本与GPS主节点相当。
   - 时间从节点（per machine）：每台机器运行一个timeslave守护进程，负责同步本地时钟。
3. 同步与误差检测机制：
   - 主节点自检：定期交叉校验彼此的时间，同时检查自身时间源与本地时钟的推进速率，若偏差过大则自动下线。
   - 从节点同步：采用Marzullo算法（检测并剔除“异常值”），轮询多个主节点（含本地数据中心GPS主节点、远程数据中心GPS主节点、原子钟主节点），将本地时钟同步到可信主节点的时间。
   - 机器级监控：若本地时钟频率偏移超出硬件规格与运行环境的最坏情况边界，该机器会被下线，避免影响全局时间精度。
4. 不确定性（ε）控制：
   - ε定义：瞬时误差边界为区间宽度的一半（`(latest - earliest)/2`），平均误差边界为`ε`。
   - 常规误差来源：
     - 本地时钟漂移：默认按200微秒/秒的最坏情况计算，30秒同步周期内贡献6ms误差。
     - 主节点通信延迟：约1ms，叠加漂移误差后，常规ε为1-7ms，多数时间稳定在4ms。
   - 异常误差场景：时间主节点维护、网络拥堵等可能导致ε临时升高（如单数据中心2个主节点下线时，ε会短期上升），需通过运维优化减少此类情况。

## TrueTime对Spanner核心特性的支撑作用

1. 基础支撑：为事务分配**全局有意义的提交时间戳**，确保时间戳能反映事务序列化顺序，且满足外部一致性（线性izability）——若事务`T1`提交在`T2`开始前，则`T1`的提交时间戳 `< T2`的提交时间戳，这是Spanner成为首个全球级外部一致性系统的关键。
2. 对核心功能的具体赋能：
   - 外部一致性事务：通过“提交等待（Commit Wait）”机制，协调者Leader需等待`TT.after(s_i)`为真（`s_i`为事务`T_i`的提交时间戳），确保`s_i < t_abs(e_i^commit)`（`e_i^commit`为`T_i`的提交事件），结合时间戳分配规则（`s_i ≥ TT.now().latest`），保证事务间时序与绝对时间一致。
   - 无锁只读事务：只读事务可选择“满足外部一致性的最老时间戳”（如单Paxos组的`LastTS()`，即该组最后提交事务的时间戳），无需加锁，依托TrueTime确保所选时间戳能反映全局时序，避免读取到“未来”数据。
   - 原子 schema 变更：将schema变更的生效时间戳注册为未来时间，依托TrueTime的确定性（`TT.after(t)`可精准判断生效时间是否到来），让读写操作根据自身时间戳与变更时间戳的关系，自动决定是否阻塞，实现跨全球节点的原子变更。
3. 性能关联：TrueTime的不确定性ε直接影响Spanner性能——若ε过大，Spanner需延长“提交等待”时间以保证一致性，导致写延迟升高；当前实现通过双时间源与优化同步机制，将ε控制在10ms以内，确保性能可接受。

# Concurrency Control

## Timestamp Management

### 核心前提：明确操作类型与特性

Spanner 支持的操作类型可分为**读写事务（Read-Write Transaction）**、**只读事务（Read-Only Transaction）**、**快照读（Snapshot Read）** 三类，不同操作的时间戳管理逻辑与并发控制策略差异显著，具体特性如下（对应文档中 Table 2）：

1. **读写事务（RW Transaction）**
    
    - 本质：支持读写操作的事务，需通过两阶段锁（2PL）实现并发控制，确保写操作的原子性与一致性。
    - 关键特点：必须在 Paxos Leader 副本执行（需获取锁），时间戳分配与 Paxos 提交写操作强绑定，内部自动重试（客户端无需处理重试逻辑）。
2. **只读事务（RO Transaction）**
    
    - 本质：预声明无写操作的 “快照隔离事务”，并非 “无写操作的读写事务”，核心优势是**无锁执行**，不阻塞后续写操作。
    - 关键特点：分两阶段执行 —— 先分配读时间戳（`s_read`），再以快照读方式在 “足够新” 的副本上执行读操作；时间戳分配需保证外部一致性，副本可选择 Leader（分配时间戳）或其他符合条件的副本（执行读）。
3. **快照读（Snapshot Read）**
    
    - 本质：针对 “过去数据” 的非阻塞读，无需锁机制，支持客户端自定义时间戳或时间戳范围。
    - 关键特点：客户端可直接指定读时间戳，或提供 “最大陈旧度上限” 由 Spanner 自动选时间戳；执行时仅需访问 “足够新” 的副本，无需依赖 Leader，且一旦时间戳确定，数据读取结果唯一（除非数据被垃圾回收）。

### 时间戳分配规则：按操作类型差异化设计

Spanner 为不同操作分配时间戳的核心目标是**保障外部一致性**（若事务 `T1` 提交在 `T2` 开始前，则 `T1` 的提交时间戳 < `T2` 的提交时间戳），同时兼顾性能（如减少锁阻塞、支持多副本读），具体规则分三类：

#### 读写事务（RW Transaction）：基于 Paxos 提交的时间戳绑定

读写事务的时间戳分配与 Paxos 协议深度耦合，核心依赖 “单调性不变式” 与 “外部一致性不变式”，具体流程如下：

- **时间戳来源**：事务的提交时间戳 = Paxos 协议为 “事务提交记录” 分配的时间戳。由于 Paxos 写操作按顺序应用，且同一 Paxos 组内时间戳单调递增（即使 Leader 变更，也通过 “Leader 租约区间不重叠” 保障），事务时间戳天然满足 “单调递增”。
- **分配时机**：在事务获取所有锁后、释放锁前分配 —— 确保锁持有期间无其他事务干扰，避免时间戳冲突。
- **外部一致性保障**：通过 “Start 规则” 与 “Commit Wait 规则” 双重约束：
    1. **Start 规则**：协调者 Leader 为事务分配的时间戳 `s_i`，必须 ≥ `TT.now().latest`（`TT.now().latest` 是协调者接收提交请求时的 “最晚可能时间”），确保时间戳不早于事务提交请求的实际到达时间。
    2. **Commit Wait 规则**：协调者 Leader 需等待 `TT.after(s_i)` 为真（即 `s_i` 已确定落在过去），才允许客户端看到事务结果，确保 `s_i` < 事务实际提交的绝对时间（`t_abs(e_i^commit)`）。
        
        结合两规则，若 `T1` 提交在 `T2` 开始前，必有 `s_1 < t_abs(e_1^commit) < t_abs(e_2^start) ≤ s_2`，最终 `s_1 < s_2`，满足外部一致性。

#### 只读事务（RO Transaction）：无锁的快照时间戳

只读事务的时间戳分配需兼顾 “外部一致性” 与 “无锁性能”，核心是选择 “满足一致性的最老时间戳” 以减少阻塞，具体分两种场景：

- **单 Paxos 组场景**：若事务仅涉及一个 Paxos 组，直接取该组 “最后一次提交写操作的时间戳（`LastTS()`）” 作为 `s_read`。此时无未提交事务（或未提交事务的准备时间戳 > `LastTS()`），`s_read` 能覆盖所有已提交事务，天然满足外部一致性，且无需等待。
- **多 Paxos 组场景**：有两种分配策略：
    1. **协商策略**：与所有涉及的 Paxos 组 Leader 通信，取各组长的 `LastTS()` 最大值作为 `s_read`，确保覆盖所有组的已提交事务。
    2. **简化策略（当前实现）**：直接取 `TT.now().latest` 作为 `s_read`，虽可能因副本 `t_safe` 未追上 `s_read` 导致短暂阻塞，但避免了跨组协商的网络延迟，更易实现。

#### 快照读（Snapshot Read）：客户端主导的时间戳

快照读的时间戳由客户端或 Spanner 灵活决定，核心是 “基于 `t_safe` 确保数据有效性”：

- **客户端指定时间戳**：客户端直接提供读时间戳 `t`，Spanner 仅在副本 `t_safe ≥ t` 时执行读操作（`t_safe` 是副本能提供的 “最新安全时间”），确保读取数据不包含未提交事务或未来数据。
- **客户端指定陈旧度上限**：客户端提供 “最大可接受陈旧度”（如 “不超过 5 秒”），Spanner 自动选择 ≤ 当前时间 - 陈旧度的最大时间戳作为 `t`，平衡延迟与数据新鲜度。

### 关键支撑：安全时间（`t_safe`）机制

无论只读事务还是快照读，都需依赖 “安全时间（`t_safe`）” 判断副本是否 “足够新” 以服务读请求，`t_safe` 是副本能提供的 “最大安全读时间戳”，即读请求时间戳 `t ≤ t_safe` 时，读取结果必为已提交的一致数据。

#### `t_safe` 的计算逻辑

`t_safe = min(t_safe^Paxos, t_safe^TM)`，其中：

- **`t_safe^Paxos`**：副本已应用的 “最高 Paxos 写操作时间戳”。由于 Paxos 写操作按时间戳递增顺序应用，`t_safe^Paxos` 之后不会再出现 ≤ 该时间戳的写操作，确保读操作不会遗漏已提交数据。
- **`t_safe^TM`**：事务管理器（TM）维护的 “安全时间”，与未提交事务（处于两阶段提交 “准备阶段” 的事务）相关：
    - 若无未提交事务，`t_safe^TM = ∞`（副本可服务任意时间戳的读请求）。
    - 若有未提交事务，`t_safe^TM = min(s_i,g^prepare) - 1`（`s_i,g^prepare` 是所有未提交事务在该 Paxos 组的 “准备时间戳”）。由于事务最终提交时间戳 ≥ `s_i,g^prepare`，`t_safe^TM` 确保读操作不会包含未确定是否提交的事务数据。

#### `t_safe` 的优化：解决 “假冲突” 与 “无写阻塞”

文档中提到 `t_safe` 存在两个原生问题，需通过细粒度优化解决：

- **问题 1**：单个未提交事务会阻塞所有读请求（即使读请求与未提交事务无键冲突）。
    
    **优化**：在锁表中存储 “键范围 → 未提交事务准备时间戳” 的映射，`t_safe^TM` 仅针对读请求涉及的键范围计算，非冲突读请求可正常执行。
- **问题 2**：无 Paxos 写操作时，`t_safe^Paxos` 无法推进，导致快照读无法访问 “最新时间戳”。
    
    **优化**：Paxos Leader 维护 `MinNextTS(n)` 映射（`n` 为 Paxos 序列号，`MinNextTS(n)` 是序列号 `n+1` 写操作的最小可能时间戳），副本应用完序列号 `n` 后，可将 `t_safe^Paxos` 推进至 `MinNextTS(n) - 1`，即使无新写操作，`t_safe` 也能定期（默认每 8 秒）推进。

### 核心价值：时间戳管理如何支撑 Spanner 特性

1. **外部一致性事务**：通过读写事务的 “Commit Wait 规则” 与时间戳单调递增，确保全球分布式场景下事务时序与绝对时间一致。
2. **无锁只读事务**：只读事务的时间戳分配不依赖锁，且可在任意 `t_safe` 足够的副本执行，避免阻塞写操作，提升读吞吐量。
3. **非阻塞快照读**：基于 `t_safe` 机制，客户端可读取任意过去时间戳的一致数据，支持历史数据审计、跨区域低延迟读等场景。
4. **故障恢复友好**：一旦时间戳确定，读操作可在任意副本重试（只需复用时间戳与读位置），无需客户端缓存结果，提升系统可用性。

## Details

### 读写事务

| 流程阶段       | 关键操作主体           | 核心步骤                                                                                                      | 设计目的                                                  |
| ---------- | ---------------- | --------------------------------------------------------------------------------------------------------- | ----------------------------------------------------- |
| 事务初始化      | 客户端+Paxos Leader | 1. 客户端缓冲所有写操作，事务内读操作看不到未提交写（未分配时间戳）；<br>2. 读操作发往Leader，Leader加读锁+用wound-wait防死锁；<br>3. 客户端发keepalive防事务超时 | 1. 避免客户端-服务器冗余传输；<br>2. 防止长事务死锁与回滚浪费；<br>3. 保障长事务正常执行 |
| 两阶段提交-准备阶段 | 非协调者参与者Leader    | 1. 加写锁；<br>2. 分配准备时间戳（>历史事务时间戳，保单调）；<br>3. Paxos持久化准备记录；<br>4. 通知协调者准备就绪                                  | 确保参与者本地可提交，保时间戳单调，防数据丢失                               |
| 两阶段提交-决策阶段 | 协调者Leader        | 1. 加写锁，跳过准备阶段；<br>2. 分配提交时间戳s（≥所有准备时间戳、>TT.now().latest、>历史时间戳）；<br>3. Paxos持久化提交/回滚记录                    | 保外部一致性与时间戳单调，优化提交流程                                   |
| 提交等待       | 协调者Leader        | 等待TT.after(s)为真（s确定在过去），等待时长≥2ε，可与Paxos同步重叠                                                               | 满足Commit Wait规则，确保s<事务实际提交时间，保外部一致性                   |
| 结果同步与锁释放   | 协调者+所有参与者        | 1. 协调者通知客户端与参与者结果；<br>2. 参与者Paxos持久化结果，以s应用写操作；<br>3. 所有节点释放锁                                             | 确保所有副本数据一致，释放资源供后续事务使用                                |

## 只读事务,模式更改事务

| 事务类型                                        | 核心阶段/步骤                                                                                                                                                                                                                                                                 | 关键规则与约束                                                                                                                                                    | 设计优势与目标                                                                                                |
| ------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ |
| **4.2.2 只读事务（RO Transaction）**              | 1. **确定事务范围**：需声明“读键范围表达式”（scope），总结事务要读取的所有键，独立查询由Spanner自动推断范围；<br>2. **分配读时间戳（s_read）**：<br>   - 单Paxos组：发往对应组Leader，分配`LastTS()`（组内最后提交写操作时间戳）；<br>   - 多Paxos组：客户端直接用`TT.now().latest`（简化实现，避免跨组协商）；<br>3. **执行读操作**：在`t_safe ≥ s_read`的任意副本执行无锁读（`t_safe`为副本安全时间） | 1. 时间戳需保外部一致性：单组用`LastTS()`确保不遗漏已提交数据，多组用`TT.now().latest`符合“Start规则”；<br>2. 读操作依赖`safe time`：仅在副本数据足够新时执行，避免读过期/未提交数据；<br>3. 预声明无写操作：区别于“无写的读写事务”，确保可无锁执行 | 1. 无锁执行：不阻塞写操作，提升并发读性能；<br>2. 灵活选副本：读可在非Leader副本执行，降低Leader负载；<br>3. 免重试缓冲：时间戳确定后结果唯一，客户端可跨副本重试，无需缓冲结果 |
| **4.2.3 模式更改事务（Schema-Change Transaction）** | 1. **预分配未来时间戳**：显式为模式更改分配未来时间戳`t`，在准备阶段注册该时间戳；<br>2. **同步模式更改**：跨所有Paxos组（可能数百万个）注册`t`；<br>3. **读写操作适配**：<br>   - 读写时间戳＜t：正常执行，不依赖新模式；<br>   - 读写时间戳＞t：阻塞至模式更改完成，再执行；<br>4. **原子生效**：所有节点在时间`t`后统一使用新模式                                                                 | 1. 时间戳依赖TrueTime：无TrueTime则“未来时间`t`”无意义，无法同步生效时机；<br>2. 非阻塞设计：模式更改不阻塞现有读写（仅时间戳＞t的操作短暂阻塞）；<br>3. 跨组原子性：即使涉及数百万Paxos组，也能在`t`后统一生效                            | 1. 支持大规模原子更改：解决传统事务无法跨百万组执行的问题；<br>2. 低干扰：不阻塞多数现有操作，保障业务连续性；<br>3. 全局一致性：所有节点同步生效新模式，避免模式不一致导致的查询错误    |

# AI summarize

### Spanner 核心要点（精简）
- **定位**：Google 推出的全球分布式数据库，首款支持**外部一致性事务**的系统。
- **核心创新**：**TrueTime API**（结合 GPS 与原子钟），将时钟不确定性控制在 10ms 以内，为事务提供全局可信时间戳。
- **数据模型**：半关系型表结构，支持 SQL 查询、多版本数据（自动时间戳）、目录（Directory）作为数据复制与迁移的基本单位。
- **复制与一致性**：基于 **Paxos 协议**实现多副本同步复制，通过**两阶段提交（2PC）** 支持跨节点事务。
- **事务类型**：
  - **读写事务**：严格依赖锁表与 Paxos，保证原子性。
  - **只读事务**：无锁执行，通过安全时间戳（如 `LastTS()`）实现外部一致性。
  - **快照读**：支持读取历史数据，无需阻塞写操作。
- **性能表现**：单副本写入延迟约 14.4ms，读吞吐量随副本数线性增长（如 5 副本达 50K ops/sec）。

---

### Spanner 架构设计可借鉴之处
1. **TrueTime 机制解决时钟同步难题**  
   - 通过 **GPS + 原子钟** 双源冗余，暴露时钟不确定性（ε），并利用 `Commit Wait` 规则（等待 `TT.after(s)`）保障外部一致性。  
   - **借鉴价值**：分布式系统可引入类似硬件辅助的时间API，避免依赖单一时钟源，提升全局时序可靠性。

2. **目录（Directory）作为数据管理单元**  
   - 目录是**连续键前缀的集合**，作为复制、迁移和局部性优化的基本单位。  
   - **借鉴价值**：通过数据分组（如按业务前缀）控制物理分布，减少跨节点访问，同时支持在线迁移（Movedir）实现动态负载均衡。

3. **Paxos 与事务的深度集成**  
   - 每个 Tablet 绑定一个 Paxos 状态机，通过 **Leader 长租约（10秒）** 减少选举开销，并在 Paxos 上运行 2PC 以规避协调者单点故障。  
   - **借鉴价值**：将共识协议（如 Raft/Paxos）与事务管理器耦合，可在保证一致性的同时提升可用性。

4. **分层并发控制与时间戳管理**  
   - **读写事务**：严格按 Paxos 顺序分配时间戳，结合两阶段锁防冲突。  
   - **只读事务**：通过 `t_safe` 机制（安全时间）实现无锁快照读，允许非 Leader 副本服务读请求。  
   - **借鉴价值**：区分读写路径可显著提升读吞吐量，尤其适合读多写少的场景（如内容平台、审计系统）。

5. **原子模式变更与未来时间戳**  
   - 模式变更事务预注册**未来时间戳**，依赖 TrueTime 在全局节点同步生效，避免阻塞运行时操作。  
   - **借鉴价值**：通过时间戳规划大规模元数据变更，实现无中断升级，适用于微服务架构或数据库 schema 演进。

6. **灵活的数据模型与局部性优化**  
   - 通过 **INTERLEAVE IN** 语法将关联数据（如用户表与订单表）物理共置，减少分布式查询延迟。  
   - **借鉴价值**：在分布式数据库中显式定义数据亲和性，可优化 JOIN 性能与跨境访问成本。

> 总结：Spanner 的核心启示在于**通过硬件增强时间语义（TrueTime）实现强一致性**，并围绕**目录化数据组织**、**Paxos+2PC 事务融合**及**细粒度并发控制**构建可扩展架构。这些设计对开发全球级分布式数据库、跨地域存储系统具有重要参考意义。
# Reference
